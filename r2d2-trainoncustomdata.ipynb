{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/naver/r2d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cd r2d2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, pdb\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tools import common, trainer\n",
    "from tools.dataloader import *\n",
    "from nets.patchnet import *\n",
    "from nets.losses import *\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.notebook import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     DataPreparation\n",
    "# except NameError:\n",
    "#     os.system(\"cp -r /kaggle/input/naturalimages/ /tmp/dataset\")\n",
    "#     os.system(\"ln -s /tmp/dataset /kaggle/working/r2d2/data\")\n",
    "#     DataPreparation = True\n",
    "# else:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cp -r /kaggle/input/naturalimages/ /tmp/dataset\n",
    "# !ln -s /tmp/dataset /kaggle/working/r2d2/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ls /tmp/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from  datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StillTransform (object):\n",
    "    \"\"\" Takes and return an image, without changing its shape or geometry.\n",
    "    \"\"\"\n",
    "    def _transform(self, img):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        img = F.grab_img(inp)\n",
    "\n",
    "        # transform the image (size should not change)\n",
    "        try:\n",
    "            img = self._transform(img)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        return F.update_img_and_labels(inp, img, persp=(1,0,0,0,1,0,0,0))\n",
    "\n",
    "    \n",
    "class PixelSpeckleNoise (StillTransform):\n",
    "    \"\"\" Takes an image, and add random white noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, var=.05, seed=None):\n",
    "        StillTransform.__init__(self)\n",
    "        assert 0 <= var < 1\n",
    "        self.var = var\n",
    "        self.seed = seed\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PixelSpeckleNoise(%g)\" % self.var\n",
    "    \n",
    "    def normalize(self, img,minimum=0, maximum=1):\n",
    "        img_max = np.max(img)\n",
    "        img_min = np.min(img)\n",
    "        return (img-img_min)/np.abs(img_max-img_min)*(maximum-minimum)+minimum\n",
    "\n",
    "    def _transform(self, img):\n",
    "        normalized_img = self.normalize(img)\n",
    "        upper_band = (12*self.var)**.5\n",
    "        np.random.seed(self.seed)\n",
    "        noise = np.random.uniform(-upper_band/2,upper_band/2,size=img.shape)\n",
    "        noisy_img = normalized_img*(1+noise)\n",
    "        noisy_img = np.clip(noisy_img,0,1)\n",
    "        ret_val = self.normalize(noisy_img,maximum=255)\n",
    "        return  Image.fromarray(np.uint8(ret_val))\n",
    "    \n",
    "class PixelNoise (StillTransform):\n",
    "    \"\"\" Takes an image, and add random white noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, ampl=20):\n",
    "        StillTransform.__init__(self)\n",
    "        assert 0 <= ampl < 255\n",
    "        self.ampl = ampl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PixelNoise(%g)\" % self.ampl\n",
    "\n",
    "    def _transform(self, img):\n",
    "        img = np.float32(img)\n",
    "        img += np.random.uniform(0.5-self.ampl/2, 0.5+self.ampl/2, size=img.shape)\n",
    "        return Image.fromarray(np.uint8(img.clip(0,255)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_net = \"Quad_L2Net_ConfCFS()\"\n",
    "\n",
    "toy_db_debug = \"\"\"SyntheticPairDataset(\n",
    "    ImgFolder('imgs'), \n",
    "            'RandomScale(256,1024,can_upscale=True)', \n",
    "            'RandomTilting(0.5), PixelSpeckleNoise(.5)')\"\"\"\n",
    "\n",
    "db_web_images = \"\"\"SyntheticPairDataset(\n",
    "    web_images, \n",
    "        'RandomScale(256,1024,can_upscale=True)',\n",
    "        'RandomTilting(0.5), PixelSpeckleNoise(.5)')\"\"\"\n",
    "\n",
    "db_aachen_images = \"\"\"SyntheticPairDataset(\n",
    "    aachen_db_images, \n",
    "        'RandomScale(256,1024,can_upscale=True)', \n",
    "        'RandomTilting(0.5), PixelSpeckleNoise(.5)')\"\"\"\n",
    "\n",
    "db_aachen_style_transfer = \"\"\"TransformedPairs(\n",
    "    aachen_style_transfer_pairs,\n",
    "            'RandomScale(256,1024,can_upscale=True), RandomTilting(0.5), PixelSpeckleNoise(.5)')\"\"\"\n",
    "\n",
    "db_aachen_flow = \"aachen_flow_pairs\"\n",
    "\n",
    "\n",
    "db_sar_images = \"\"\"SyntheticPairDataset(\n",
    "    sar_db_images, \n",
    "        'RandomScale(256,1024,can_upscale=True)', \n",
    "        'RandomTilting(0.5), PixelSpeckleNoise(.5)')\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "default_dataloader = \"\"\"PairLoader(CatPairDataset(`data`),\n",
    "    scale   = 'RandomScale(256,1024,can_upscale=True)',\n",
    "    distort = 'ColorJitter(0.2,0.2,0.2,0.1)',\n",
    "    crop    = 'RandomCrop(192)')\"\"\"\n",
    "\n",
    "default_sampler = \"\"\"NghSampler2(ngh=7, subq=-8, subd=1, pos_d=3, neg_d=5, border=16,\n",
    "                            subd_neg=-8,maxpool_pos=True)\"\"\"\n",
    "\n",
    "default_loss = \"\"\"MultiLoss(\n",
    "        1, ReliabilityLoss(`sampler`, base=0.5, nq=20),\n",
    "        1, CosimLoss(N=`N`),\n",
    "        1, PeakyLoss(N=`N`))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sources = dict(\n",
    "#     D = toy_db_debug,\n",
    "#     W = db_web_images,\n",
    "#     A = db_aachen_images,\n",
    "#     F = db_aachen_flow,\n",
    "#     S = db_aachen_style_transfer,\n",
    "    X = db_sar_images\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyTrainer(trainer.Trainer):\n",
    "    \"\"\" This class implements the network training.\n",
    "        Below is the function I need to overload to explain how to do the backprop.\n",
    "    \"\"\"\n",
    "    def forward_backward(self, inputs):\n",
    "        output = self.net(imgs=[inputs.pop('img1'),inputs.pop('img2')])\n",
    "        allvars = dict(inputs, **output)\n",
    "        loss, details = self.loss_func(**allvars)\n",
    "        if torch.is_grad_enabled(): loss.backward()\n",
    "        return loss, details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_network(model_fn): \n",
    "    checkpoint = torch.load(model_fn)\n",
    "    print(\"\\n>> Creating net = \" + checkpoint['net']) \n",
    "    net = eval(checkpoint['net'])\n",
    "    nb_of_weights = common.model_size(net)\n",
    "    print(f\" ( Model size: {nb_of_weights/1000:.0f}K parameters )\")\n",
    "\n",
    "    # initialization\n",
    "    weights = checkpoint['state_dict']\n",
    "    net.load_state_dict({k.replace('module.',''):v for k,v in weights.items()})\n",
    "    return net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘trained_models’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = \"./trained_models\"\n",
    "gpu = 0\n",
    "train_data = \"X\"\n",
    "data_loader = default_dataloader\n",
    "threads = 8\n",
    "batch_size = 8\n",
    "net = default_net\n",
    "sampler = default_sampler\n",
    "N = patch_size = 16 \n",
    "loss = default_loss\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-4\n",
    "epochs = 10\n",
    "network_path = \"./models/faster2d2_WASF_N16.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HOSTNAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c94a3bd06749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miscuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_set_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/common.py\u001b[0m in \u001b[0;36mtorch_set_gpu\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         assert cuda and torch.cuda.is_available(), \"%s has GPUs %s unavailable\" % (\n\u001b[0;32m---> 32\u001b[0;31m             os.environ['HOSTNAME'],os.environ['CUDA_VISIBLE_DEVICES'])\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# speed-up cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# even more speed-up?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HOSTNAME'"
     ]
    }
   ],
   "source": [
    "iscuda = common.torch_set_gpu(gpu)\n",
    "common.mkdir_for(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = [data_sources[key] for key in train_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PairLoader(CatPairDataset(SyntheticPairDataset(    sar_db_images,         'RandomScale(256,1024,can_upscale=True)',         'RandomTilting(0.5), PixelSpeckleNoise(.5)')),    scale   = 'RandomScale(256,1024,can_upscale=True)',    distort = 'ColorJitter(0.2,0.2,0.2,0.1)',    crop    = 'RandomCrop(192)')\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.replace('`data`',','.join(db)).replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image database = PairLoader\n",
      "CatPairDataset(Dataset: SyntheticPairDataset   8000 images and pairs   root: data/sar...   Scale: Compose(    RandomScale(0))   Distort: Compose(    RandomTilt(0.5, 'all')    PixelSpeckleNoise(0.5)) )  npairs: 8000\n",
      "  Distort: ColorJitter(0.2,0.2,0.2,0.1), \n",
      "  Crop: RandomCrop((192, 192)), \n",
      "  Norm: ToTensor(),  Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "db = [data_sources[key] for key in train_data]\n",
    "db = eval(data_loader.replace('`data`',','.join(db)).replace('\\n',''))\n",
    "print(\"Training image database =\", db)\n",
    "loader = threaded_loader(db, False, threads, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/dataloader.py\", line 70, in __getitem__\n    img_a, img_b, metadata = self.dataset.get_pair(i, self.what)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/datasets/pair_dataset.py\", line 268, in get_pair\n    return self.datasets[b].get_pair(i, output)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/datasets/pair_dataset.py\", line 152, in get_pair\n    scaled_and_distorted_image = self.distort(\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/transforms.py\", line 387, in __call__\n    img = self._transform(img)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/transforms.py\", line 434, in _transform\n    noise = np.random.uniform(-upper_band / 2, upper_band / 2, size=img.shape)\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\", line 541, in __getattr__\n    raise AttributeError(name)\nAttributeError: shape\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-c4cc5574b32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/dataloader.py\", line 70, in __getitem__\n    img_a, img_b, metadata = self.dataset.get_pair(i, self.what)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/datasets/pair_dataset.py\", line 268, in get_pair\n    return self.datasets[b].get_pair(i, output)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/datasets/pair_dataset.py\", line 152, in get_pair\n    scaled_and_distorted_image = self.distort(\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/transforms.py\", line 387, in __call__\n    img = self._transform(img)\n  File \"/run/media/javid/FC201C01201BC216/arco_home/ml/projects/p12_r2d2_local_descriptor/r2d2-master/tools/transforms.py\", line 434, in _transform\n    noise = np.random.uniform(-upper_band / 2, upper_band / 2, size=img.shape)\n  File \"/home/javid/.conda/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\", line 541, in __getattr__\n    raise AttributeError(name)\nAttributeError: shape\n"
     ]
    }
   ],
   "source": [
    "for a in loader:\n",
    "    print(a)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e734f8aca5ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T21:53:45.284794Z",
     "iopub.status.busy": "2022-02-08T21:53:45.284137Z",
     "iopub.status.idle": "2022-02-08T21:53:45.321532Z",
     "shell.execute_reply": "2022-02-08T21:53:45.32077Z",
     "shell.execute_reply.started": "2022-02-08T21:53:45.284752Z"
    }
   },
   "outputs": [],
   "source": [
    "net = load_network(network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T21:53:45.485416Z",
     "iopub.status.busy": "2022-02-08T21:53:45.485168Z",
     "iopub.status.idle": "2022-02-08T21:53:45.489551Z",
     "shell.execute_reply": "2022-02-08T21:53:45.488679Z",
     "shell.execute_reply.started": "2022-02-08T21:53:45.485389Z"
    }
   },
   "outputs": [],
   "source": [
    "# # initialization\n",
    "# pretrained = \"./models/faster2d2_WASF_N16.pt\"\n",
    "# checkpoint = torch.load(pretrained, lambda a,b:a)\n",
    "# net.load_pretrained(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Creating loss = MultiLoss(\n",
      "        1, ReliabilityLoss(NghSampler2(ngh=7, subq=-8, subd=1, pos_d=3, neg_d=5, border=16,\n",
      "                            subd_neg=-8,maxpool_pos=True), base=0.5, nq=20),\n",
      "        1, CosimLoss(N=16),\n",
      "        1, PeakyLoss(N=16))\n"
     ]
    }
   ],
   "source": [
    "# create losses\n",
    "loss = loss.replace('`sampler`',sampler).replace('`N`',str(patch_size))\n",
    "print(\"\\n>> Creating loss = \" + loss)\n",
    "loss = eval(loss.replace('\\n',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3c8c8a0cb059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m optimizer = optim.Adam( [p for p in net.parameters() if p.requires_grad], \n\u001b[0m\u001b[1;32m      3\u001b[0m                         lr=learning_rate, weight_decay=weight_decay)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = optim.Adam( [p for p in net.parameters() if p.requires_grad], \n",
    "                        lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "train = MyTrainer(net, loader, loss, optimizer)\n",
    "if iscuda: train = train.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T21:53:46.890952Z",
     "iopub.status.busy": "2022-02-08T21:53:46.890147Z",
     "iopub.status.idle": "2022-02-08T21:54:13.154465Z",
     "shell.execute_reply": "2022-02-08T21:54:13.151761Z",
     "shell.execute_reply.started": "2022-02-08T21:53:46.890903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop #\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n>> Starting epoch {epoch}...\")\n",
    "    train()\n",
    "\n",
    "print(f\"\\n>> Saving model to {save_path}\")\n",
    "torch.save({'net': args.net, 'state_dict': net.state_dict()}, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
